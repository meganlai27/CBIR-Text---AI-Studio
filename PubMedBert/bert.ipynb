{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\megan\\OneDrive\\Desktop\\Break Through Tech AI\\CBIR-Text---AI-Studio\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\megan\\.cache\\huggingface\\hub\\models--microsoft--BiomedNLP-BiomedBERT-base-uncased-abstract. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\megan\\OneDrive\\Desktop\\Break Through Tech AI\\CBIR-Text---AI-Studio\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"fill-mask\", model=\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\megan\\OneDrive\\Desktop\\Break Through Tech AI\\CBIR-Text---AI-Studio\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\megan\\OneDrive\\Desktop\\Break Through Tech AI\\CBIR-Text---AI-Studio\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Get data from json file\n",
    "with open('./pubmed_captions.json', 'r') as f:\n",
    "    pubmed = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"basophilic\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "query_embedding = outputs.hidden_states[-1][:, 0, :]\n",
    "# outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acinar cell carcinoma. Partially intact tissue fragment with neoplastic cells has moderately hyperchromatic, uniform nuclei and delicate basophilic cytoplasm. A lack of single dispersed cells may help to differentiate this from a pancreatic neuroendocrine tumor (Diff-Quik stain).\n"
     ]
    }
   ],
   "source": [
    "# Test basophilic\n",
    "basophilic_caption = pubmed.get(\"29e5289d-121b-4c14-bf9f-2d9ab26a2eba\")\n",
    "print(basophilic_caption)\n",
    "\n",
    "inputs = tokenizer(basophilic_caption, return_tensors=\"pt\")\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "basophilic_embedding = outputs.hidden_states[-1][:, 0, :]\n",
    "\n",
    "# Non basophilic\n",
    "pneumonia_caption = pubmed.get(\"cc262f9a-d835-4660-befb-248ee3e2106a\")\n",
    "inputs = tokenizer(pneumonia_caption, return_tensors=\"pt\")\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "pneumonia_embeddings = outputs.hidden_states[-1][:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CosineSimilarity\n",
    "from transformers import CLIPTokenizer, CLIPModel, CLIPTextModel\n",
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "import faiss # For dot product and L2 similarity\n",
    "cossim = CosineSimilarity(dim=0, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_caption(key):\n",
    "    caption = pubmed.get(key)\n",
    "    inputs = tokenizer(caption, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "    embedding = outputs.hidden_states[-1][:, 0, :]\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(v1, v2):\n",
    "  result = F.cosine_similarity(torch.tensor(v1), torch.tensor(v2))\n",
    "  # print(result)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "tensor([0.8483]) tensor([0.8477])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\megan\\AppData\\Local\\Temp\\ipykernel_20908\\127360065.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result = F.cosine_similarity(torch.tensor(v1), torch.tensor(v2))\n"
     ]
    }
   ],
   "source": [
    "#Cosine similarity \n",
    "baso_eval = cos_sim(query_embedding, basophilic_embedding)\n",
    "other_eval = cos_sim(query_embedding, pneumonia_embeddings)\n",
    "\n",
    "print(baso_eval, other_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3309\n"
     ]
    }
   ],
   "source": [
    "caption_embeddings = {}\n",
    "keys = [k for k in pubmed.keys()]\n",
    "print(len(keys))\n",
    "\n",
    "for k in keys[:200]:\n",
    "    embedding = embed_caption(k)\n",
    "    caption_embeddings[k] = embedding.tolist()\n",
    "\n",
    "with open('bert_embeddings.json', 'w') as f:\n",
    "    json.dump(caption_embeddings, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3309\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get data from json file\n",
    "with open('./bert_embeddings.json', 'r') as f:\n",
    "    bert_embeddings = json.load(f)\n",
    "\n",
    "print(len(bert_embeddings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embeddings = {}\n",
    "for k in keys[3200:]:\n",
    "    embedding = embed_caption(k)\n",
    "    new_embeddings[k] = embedding.tolist()\n",
    "\n",
    "caption_embeddings.update(new_embeddings)\n",
    "with open('bert_embeddings.json', 'w') as f:\n",
    "    json.dump(caption_embeddings, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in keys[400:600]:\n",
    "    embedding = embed_caption(k)\n",
    "    caption_embeddings[k] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in keys[600:800]:\n",
    "    embedding = embed_caption(k)\n",
    "    caption_embeddings[k] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in keys[:200]:\n",
    "    embedding = embed_caption(k)\n",
    "    caption_embeddings[k] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a query and the image data, find the n top related pathology images\n",
    "def nTopImages(query, embeddings, n=5):\n",
    "#   query_embedding = getQueryEmbedding([query])\n",
    "  keys = [\"None\"]\n",
    "  evals = [0]\n",
    "  eval_scores = {}\n",
    "  for k in embeddings.keys():\n",
    "    eval = cos_sim(embeddings[k], query)\n",
    "    for i in range(min(len(evals), n)):\n",
    "      if eval > evals[i]:\n",
    "        evals.insert(i, eval)\n",
    "        keys.insert(i, k)\n",
    "        eval_scores[k] = eval\n",
    "        break\n",
    "  return keys[:n], eval_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\megan\\AppData\\Local\\Temp\\ipykernel_20908\\127360065.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result = F.cosine_similarity(torch.tensor(v1), torch.tensor(v2))\n"
     ]
    }
   ],
   "source": [
    "top_keys, eval_results = nTopImages(query_embedding, caption_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerous small to medium-sized with thick and hyalinized walls tensor([0.9094])\n",
      "numerous cells with sebaceous differentiation tensor([0.9004])\n",
      "Plasma cells. tensor([0.8950])\n",
      "Granulomatous inflammation presents with epithelioid histiocytes, giant cells, and necrosis. tensor([0.8948])\n",
      "tumour cells exhibiting diffuse positivity with CD34 tensor([0.8945])\n"
     ]
    }
   ],
   "source": [
    "for key in top_keys:\n",
    "    print(pubmed.get(key), eval_results.get(key))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
